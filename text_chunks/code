import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
import numpy as np
from argparse import ArgumentParser
from typing import Tuple, Optional, Dict, List
import logging
import matplotlib.pyplot as plt
import seaborn as sns
from matplotlib.colors import LinearSegmentedColormap
from pathlib import Path
from dataclasses import dataclass
from scipy.stats import gaussian_kde

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


@dataclass
class ModelConfig:
    name: str
    plot_name: str
    color: str

@dataclass
class PerplexityResult:
    perplexities: List[float]
    mean: float
    ste: float


def calculate_per_token_perplexity(model: AutoModelForCausalLM, tokenizer: AutoTokenizer, input_string: str, k: int) -> \
List[float]:
    input_ids = tokenizer.encode(input_string, return_tensors="pt").to(model.device)
    num_tokens = input_ids.size(1) - 1

    perplexities = []
    for i in range(k, num_tokens):
        current_input = input_ids[:, : i + 1]
        target = input_ids[:, i + 1]

        with torch.no_grad():
            outputs = model(current_input)
            logits = outputs.logits

        loss = torch.nn.functional.cross_entropy(logits[:, -1, :], target)
        perplexities.append(loss.item())

    return perplexities


def load_model(model_config: ModelConfig, cache_dir: str) -> Tuple[AutoModelForCausalLM, AutoTokenizer]:
    logger.info(f"Loading model: {model_config.name}")
    try:
        tokenizer = AutoTokenizer.from_pretrained(model_config.name, cache_dir=cache_dir)

        if torch.cuda.is_available():
            quantization_config = BitsAndBytesConfig(
                load_in_4bit=True,
                bnb_4bit_compute_dtype=torch.bfloat16
            )
            model = AutoModelForCausalLM.from_pretrained(
                model_config.name,
                device_map="auto",
                quantization_config=quantization_config,
                cache_dir=cache_dir,
            )
        else:
            model = AutoModelForCausalLM.from_pretrained(
                model_config.name,
                device_map=None,
                torch_dtype=torch.float32,
                cache_dir=cache_dir,
            )
            model = model.to("cpu")  # Explicitly move to CPU if CUDA is not available

        return model, tokenizer
    except Exception as e:
        logger.error(f"Failed to load model {model_config.name}: {e}")
        raise

def process_text_file(file_path: Path, models: Dict[str, Tuple[AutoModelForCausalLM, AutoTokenizer]], args) -> Dict[
    str, List[float]]:
    try:
        with open(file_path, "r", encoding="utf-8") as file:
            text = file.read()
    except UnicodeDecodeError:
        logger.error(f"Failed to read file {file_path}. It may not be a valid UTF-8 encoded text file.")
        return {}

    if len(text.split()) > args.max_words:
        text = " ".join(text.split()[:args.max_words])

    results = {}
    for model_name, (model, tokenizer) in models.items():
        try:
            perplexities = calculate_per_token_perplexity(model, tokenizer, text, args.k)
            results[model_name] = perplexities
            avg_perplexity = np.mean(perplexities)
            logger.info(f"Model: {model_name}, Average Perplexity: {avg_perplexity:.4f}")
        except Exception as e:
            logger.error(f"Error processing {file_path} with model {model_name}: {e}")

    return results


def calculate_statistics(perplexities: List[float]) -> PerplexityResult:
    return PerplexityResult(
        perplexities=perplexities,
        mean=np.mean(perplexities),
        ste=np.std(perplexities) / np.sqrt(len(perplexities))
    )


def plot_perplexities(results: Dict[str, PerplexityResult], model_configs: List[ModelConfig], file_name: str,
                      plot_dir: Path):
    plt.style.use('seaborn-v0_8-whitegrid')
    sns.set_style("whitegrid", {'axes.grid': False})

    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(8, 10), dpi=300)

    # Color palette
    colors = [config.color for config in model_configs]

    # Bar plot
    means = [result.mean for result in results.values()]
    stes = [result.ste for result in results.values()]
    plot_names = [config.plot_name for config in model_configs]

    bars = ax1.bar(plot_names, means, yerr=stes, capsize=5, width=0.6, color=colors, edgecolor='black', linewidth=1.5)
    ax1.set_ylabel("Log Perplexity", fontsize=12, fontweight='bold')
    ax1.spines['top'].set_visible(False)
    ax1.spines['right'].set_visible(False)
    ax1.tick_params(axis='both', which='major', labelsize=10)

    triviality = means[1] / means[0]
    ax1.text(0.5, 0.95, f"Triviality Score: {triviality:.2f}",
             horizontalalignment='center', verticalalignment='center',
             transform=ax1.transAxes, fontsize=12, fontweight='bold',
             bbox=dict(facecolor='white', edgecolor='gray', alpha=0.8, boxstyle='round,pad=0.5'))

    # Scatter plot
    x = results[model_configs[0].name].perplexities
    y = results[model_configs[1].name].perplexities

    # Create custom colormap
    n_bins = 100
    cmap = LinearSegmentedColormap.from_list("custom", ["#FFA07A", "#98FB98", "#87CEFA"], N=n_bins)

    # Calculate point density for color mapping
    xy = np.vstack([x, y])
    z = gaussian_kde(xy)(xy)

    # Normalize z to use with n_bins
    z_normalized = (z - z.min()) / (z.max() - z.min())
    z_binned = np.floor(z_normalized * (n_bins - 1)).astype(int)

    scatter = ax2.scatter(x, y, c=z_binned, cmap=cmap, alpha=0.6, s=50, edgecolor='black', linewidth=0.5)
    ax2.set_xlabel(f"Log Perplexity of {model_configs[0].plot_name}", fontsize=12, fontweight='bold')
    ax2.set_ylabel(f"Log Perplexity of {model_configs[1].plot_name}", fontsize=12, fontweight='bold')

    max_perplexity = max(max(x), max(y))
    ax2.plot([0, max_perplexity], [0, max_perplexity], "r--", label="x=y", linewidth=2)
    ax2.legend(fontsize=10)
    ax2.spines['top'].set_visible(False)
    ax2.spines['right'].set_visible(False)
    ax2.tick_params(axis='both', which='major', labelsize=10)

    # Add colorbar
    cbar = plt.colorbar(scatter, ax=ax2, pad=0.02)
    cbar.set_label('Density', rotation=270, labelpad=15, fontsize=10, fontweight='bold')


    plt.tight_layout()
    plot_path = plot_dir / f"triviality_{file_name}.png"
    plt.savefig(plot_path, dpi=300, bbox_inches="tight")
    plt.close()


def main():
    parser = ArgumentParser()
    parser.add_argument("--k", type=int, default=100, help="Start calculating perplexity from the k-th token")
    parser.add_argument("--max_words", type=int, default=700, help="Maximum number of words to process")
    parser.add_argument("--cache_dir", type=str, default="~/", help="Cache directory for models")
    parser.add_argument("--plot_dir", type=str, default="plots", help="Directory to save plots")
    args = parser.parse_args()

    model_configs = [
        ModelConfig("meta-llama/Meta-Llama-3.1-8B", "Llama-3.1-8B", "#FF6B6B"),
        ModelConfig("meta-llama/Meta-Llama-3.1-70B", "Llama-3.1-70B", "#FF6B6B"),
    ]

    text_files = list(Path("text_chunks").glob("*"))
    plot_dir = Path(args.plot_dir)
    plot_dir.mkdir(exist_ok=True)

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    logger.info(f"Using device: {device}")

    models: Dict[str, Tuple[Optional[AutoModelForCausalLM], Optional[AutoTokenizer]]] = {}
    try:
        for config in model_configs:
            models[config.name] = load_model(config, args.cache_dir)

        for file_path in text_files:
            results = process_text_file(file_path, models, args)
            if results:
                statistics = {name: calculate_statistics(perplexities) for name, perplexities in results.items()}
                plot_perplexities(statistics, model_configs, file_path.name, plot_dir)
            else:
                logger.warning(f"Skipping plot for {file_path.name} due to processing errors.")

    except Exception as e:
        logger.error(f"An unexpected error occurred: {e}")
    finally:
        # Clean up
        for model, tokenizer in models.values():
            if model:
                del model
            if tokenizer:
                del tokenizer
        torch.cuda.empty_cache()


if __name__ == "__main__":
    main()