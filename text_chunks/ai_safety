Cybersecurity is a critical component of AI safety, and we’ve been a leader in defining the security measures that are needed for the protection of advanced AI. We will continue to take a risk-based approach to our security measures, and evolve our approach as the threat model and the risk profiles of our models change. We are pursuing expanded internal information segmentation, additional staffing to deepen around-the-clock security operations teams, and continued investment in ongoing initiatives to enhance the security of OpenAI’s research and product infrastructure.

As part of the Safety and Security Committee’s review, we identified additional opportunities for industry collaboration and information sharing to advance the security of the AI industry. For example, we are carefully evaluating the development of an Information Sharing and Analysis Center (“ISAC”) for the AI industry, to enable the sharing of threat intelligence and cybersecurity information among entities within the AI sector to enhance our collective resilience against cyber threats.

We will look for more ways to share and explain our safety work. We have long published system cards which outline the capabilities and risks of our models. The GPT-4o system card and o1-preview system card provided comprehensive details about the safety work carried out prior to releasing each model, including the results of external red teaming and scientific frontier risk evaluations(opens in a new window) within our Preparedness Framework, and an overview of the mitigations we built in to address key risk areas.

We will explore more opportunities for independent testing of our systems and will lead the push for industry-wide safety standards. For example, we’re already developing new collaborations with third-party safety organizations and non-governmental labs for independent model safety assessments. We are also working with government agencies to advance the science of AI safety. This includes working with Los Alamos National Labs—one of the United States’ leading national laboratories—to study how AI can be used safely by scientists in laboratory settings to advance bioscientific research. Additionally, we recently reached agreements with the U.S. and U.K. AI Safety Institutes to collaborate on the research of emerging AI safety risks and standards for trustworthy AI.

Ensuring the safety and security of our models involves the work of many teams across the organization. As we’ve grown and our work has become more complex, we are building upon our model launch processes and practices to establish an integrated safety and security framework with clearly defined success criteria for model launches. This framework will be based on risk assessments and approved by the Safety and Security Committee. As models get more capable, this framework will adapt to manage increased complexity and risks. To begin executing on the framework, we reorganized research, safety, and policy teams to strengthen collaboration and ensure tighter connections across the company.
